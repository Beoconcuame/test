# config.yaml - Cấu hình hoàn chỉnh ví dụ (Đã sửa lỗi num_layers cho transformer_ipa)

# --- Chế độ chạy và Task ---
run_mode: train         # train, validation, hoặc test
task: cola          # cola, sst2, mrpc, qqp, stsb, mnli, qnli, rte
seed: 42     
# --- Tokenizer ---
tokenizer: ipa  # ipa, bpe, unigram, char, wordpiece, byte
vocab_file: ./full.csv  # Đường dẫn tới file vocab (bắt buộc cho ipa)
# vocab_size: 30000     # Chỉ cần thiết nếu huấn luyện bpe/unigram/wordpiece

# --- Model ---
model: transformer_ipa

# --- Đường dẫn ---
dataset_path: ./data/   # Thư mục chứa cache/dữ liệu dataset GLUE
checkpoint_path: ./beo31.pth # Tên file checkpoint bạn đã chọn (thêm ./)
# log_file:               # Để trống để tạo tên file log tự động trong thư mục logs/

# --- Huấn luyện ---
max_len: 50             # Độ dài tối đa của chuỗi
batch_size: 32          # Kích thước batch
epochs: 40              # Số lượng epoch tối đa
lr: 0.0001          # Tốc độ học (1e-4)
weight_decay: 0      # Weight decay cho optimizer AdamW (giá trị phổ biến)
label_smoothing: 0.1 
patience: 10            # Số epoch chờ trước khi dừng sớm (Early Stopping)
resume: false           # Tiếp tục huấn luyện từ checkpoint? (true/false)

# --- Scheduler (ReduceLROnPlateau) ---
scheduler_patience: 2   # Số epoch không cải thiện trước khi giảm lr
scheduler_factor: 0.1   # Hệ số giảm lr (ví dụ: lr * 0.1)

# --- Model Hyperparameters (Quan trọng!) ---
model_params:
  # Các tham số này được truyền vào hàm __init__ của lớp model tương ứng

  # === BẮT BUỘC cho các model _ipa ===
  custom_pos_vocab_size: 8 # !!! THAY THẾ số 8 bằng kích thước vocab vị trí thực tế của bạn !!!

  # === CÁC THAM SỐ CHO transformer_ipa ===
  d_model: 256                # Kích thước embedding/ẩn (phải chia hết cho nhead)
  nhead: 8                    # Số lượng attention heads
  # num_encoder_layers: 4     # <-- Tên này bị bỏ qua theo WARNING
  num_layers: 4               # <-- SỬA LẠI THÀNH TÊN NÀY (giá trị 4 lấy từ ví dụ bị comment của bạn) - Đây là tham số __init__ yêu cầu
  dim_feedforward: 512        # Kích thước lớp feedforward (bị bỏ qua theo WARNING, có thể __init__ không cần hoặc dùng tên khác)
  dropout: 0.1                # Tỷ lệ dropout
  combine_mode: 'sum'         # Cách kết hợp embedding vị trí (bị bỏ qua theo WARNING, có thể __init__ không cần hoặc dùng tên khác)

  # === Các tham số dưới đây không dùng cho Transformer ===
  # embedding_dim: 300
  # hidden_dim: 256


# --- Cấu hình riêng cho MNLI (nếu task là mnli) ---
# mnli_eval_split: matched # 'matched' hoặc 'mismatched' - dùng cho split validation/test
